{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARS Algorithm deep working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * The below figure (1) describes the working of environments and algorithm. If this model used in a real-world robot instead of the virtual world then the input data describes as a sensor like a particular task to accomplish by the set of object example; like a robot doing movement of picking any object with hand. AI becomes a brain for a robot that gives instruction, and the output becomes the improvement or an achievement done by a robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html><center><img src ='images/figure_1.png'> </br>1. Working of Environment </center></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the above figure, AI is nothing but perception; it means it learns automatically by the weights. It does so by taking the error and backpropagating it to weights. Further, a bunch of matrix weights is then multiplied with matrix inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html><center><img src ='images/figure_2.png'> </br>2. Working of percptron </center></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Environment sends reward after every single action, and the environment code in that form that it is capable of giving appropriate rewards based on the moment od an object like half cheetah or humanoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html><center><img src ='images/figure_3.png'> </br>3. Gain more Reward</center></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoithm deep explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For adjusting the weights usually in Artificially intelligence, it uses the and gradient and descent as backpropagation but in reinforcement learning uses finite differences for updating weights. Now let's look deep into how the weights are adjusted into the model. Let's called this adjusting weights as an thata Θ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take :\n",
    "\n",
    "Θ = Weights\n",
    "δ = Random deltas or perturbations \n",
    "v = cofficent noise (just a tiny noise)\n",
    "\n",
    "Now we have matrixes of weights which received as an output from the environment also known as Θ  and then after the random perturbation is generated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html><center><img src ='images/figure_4.png'> </br>4. Deltas assign</center></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As in figure 4, there are two weights, one for positive and another for negative like this; there are several directions. Still, the direction always remains into 2n means 2*2 = 4 example; Two directions for positive and another for negative. Each direction (W1,1 (+or-) random delta) the algorithm is going to run episode with each one of the directions like positive one episode and another for negative similarly process is to be done with all directions.\n",
    "\n",
    "Step 5 : πj,k,+(x) = (Mj + νδk) then we Normalization this state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 : Sort the directions δk by max of weights we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
